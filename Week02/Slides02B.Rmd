---
title: Probability Distributions for Discrete Random Variables
author: Ben Goodrich
date: "`r format(Sys.time(), '%B %d, %Y')`"
autosize: true
header-includes:
   - \usepackage{amsmath}
   - \usepackage{color}
output:
  ioslides_presentation:
    widescreen: true
editor_options: 
  chunk_output_type: console
---
<style type="text/css">
slides > slide:not(.nobackground):after {
  content: '';
}
</style>

## Obligatory Disclosure

* Ben is an employee of Columbia University, which has received several research grants to develop Stan
* Ben is also a manager of GG Statistics LLC, which uses Stan for business purposes
* According to Columbia University 
  [policy](https://research.columbia.edu/content/conflict-interest-and-research), any such employee who 
  has any equity stake in, a title (such as officer or director) with, or is expected to earn at least 
  $\$5,000.00$ per year from a private company is required to disclose these facts in presentations

## Bernoulli Distribution

- Tuesday we talked about the Bernoulli distribution over $\Omega=\left\{ 0,1\right\}$,
  whose Probability Mass Function (PMF) is $\Pr\left(\left.x\right|\pi\right)=\pi^{x}\left(1-\pi\right)^{1-x}$, which
  depends on a possibly unknown probability parameter $\pi \in \left[0,1\right]$
- $\mathbb{E}X = 0\times\left(1-\pi\right) + 1\times\pi = \pi$
- $\mathbb{E}\left(X - \pi\right)^2 = \mathbb{E}\left[X^2\right] - \left(\mathbb{E}X\right)^2 = 
   \mathbb{E}X - \pi^2 = \pi - \pi^2 = \pi \left(1 - \pi\right)$
- You could write a model where $\pi$ is a function of predictors for each observation, 
  as in $\pi\left(z\right) = \frac{1}{1+e^{-\alpha - \beta z}}$ for a logit model

## Binomial Distribution

> - A Binomial random variable can be defined as the sum of $n$
INDEPENDENT Bernoulli random variables with the same $\pi$. What is $\Omega$ in this case?
> - What is an expression for the expectation of a Binomial random variable?
> - What is an expression for the variance of a Binomial random variable?
> - What is an expression for the PMF, $\Pr\left(\left.x\right|\pi,n=3\right)$? Hint: 8 cases to consider
    * All succeed, $\pi^3$ or all fail, $\left(1 - \pi\right)^3$
    * 1 succeeds and 2 fail $\pi^1 \left(1-\pi\right)^{3 - 1}$ 
      but there are 3 ways that could happen
    * 2 succeed and 1 fails $\pi^2 \left(1-\pi\right)^{3 - 2}$
      but there are 3 ways that could happen
    * In general, $\Pr\left(\left.x\right|n,\pi\right)={n \choose x}\pi^{x} \left(1-\pi\right)^{n-x}$, 
      where ${n \choose x}=\frac{n!}{x!\left(n-x\right)!}$ is the number of ways $x$ can occur in $n$ 
      tries (if order is unimportant) where ! indicates the factorial function

## Back to Bowling

> - Why is the binomial distribution with $n = 10$ inappropriate for the first roll of a frame of bowling?
> - Could the Bernoulli distribution be used for success in getting a strike?
> - Could the Bernoulli distribution be used for the probability of knocking over the frontmost pin?
> - If $X_i = \mathbb{I}\{\mbox{pin i is knocked down}\}$ and $\pi_i$ is the
  probability in the $i$-th Bernoulli distribution, what conceptually is
$$\Pr\left(\left.x_1\right|\pi_1\right)\prod_{i=2}^{10}
  \Pr\left(\left.x_i\right|\pi_i,X_1=x_1,X_2=x_2,\dots,X_{i-1}=x_{i-1}\right)?$$

## Poisson Distribution for Counts

- Let $n\uparrow \infty$ and let $\pi \downarrow 0$ such that $\mu = n\pi$ remains fixed and finite. What happens
  to the binomial PMF, $\Pr\left(\left.x\right|n,\pi\right)={n \choose x}\pi^{x} \left(1-\pi\right)^{n-x}$?
  
    > - $\left(1-\pi\right)^{n-x} = \left(1-\frac{\mu}{n}\right)^{n-x} = 
      \left(1-\frac{\mu}{n}\right)^n \times \left(1-\frac{\mu}{n}\right)^{-x} \rightarrow
      e^{-\mu} \times 1$
    > - ${n \choose x}\pi^{x} = \frac{n!}{x!\left(n - x\right)!} \frac{\mu^x}{n^x} = 
      \frac{n \times \left(n - 1\right) \times \left(n - 2\right) \times \dots \times \left(n - x + 1\right)} 
      {n^x} \frac{\mu^x}{x!} \rightarrow 1 \times \frac{\mu^x}{x!}$
    > - Thus, $\Pr\left(\left.x\right|\mu\right) = \frac{\mu^xe^{-\mu}}{x!}$ is the PMF of the Poisson
      distribution over $\Omega = \{0,\mathbb{Z}_+\}$, which is a common distribution for count variables

> - What is the variance of a Poisson random variable?

## Parameterized Bowling {.build}

- Bell numbers are defined as $\mathcal{B}_0 = 1$, $\mathcal{B}_1 = 1$, and else
  $\mathcal{B}_{n + 1} = \sum_{k = 0}^n {n \choose k} \mathcal{B}_k$
- Let $\Pr\left(\left.x\right|n, \Upsilon\right) = \frac{{n + \Upsilon \choose x + \Upsilon} \mathcal{B}_{x + \Upsilon}}
  {\mathcal{B}_{n + 1 + \Upsilon} \ - \sum_{k = 0}^{\Upsilon - 1} {n + \Upsilon \choose k} \mathcal{B}_k}$ where
  $\Upsilon \in \{0,\mathbb{N}_+\} = \Theta$ is an unknown
- Use a Poisson distribution with expectation $\mu$ to represent beliefs about $\Upsilon$
- Can update those beliefs with data on pins knocked down in bowling
```{r, message = FALSE}
B <- Vectorize(memoise::memoise(numbers::bell)) # makes it go faster
Pr <- function(x, n = 10, Upsilon = 0) { # probability of knocking down x out of n pins
  stopifnot(length(n) == 1, is.numeric(x), all(x == as.integer(x)))
  numer <- B(x + Upsilon) * choose(n + Upsilon, x + Upsilon) 
  denom <- B(n + 1 + Upsilon) # ^^^ choose(n, k) is n! / (k! * (n - k)!)
  if(Upsilon > 0) denom <- denom - 
    sum(choose(n + Upsilon, 0:(Upsilon - 1)) * B(0:(Upsilon - 1)))
  return(ifelse(x > n, 0, numer / denom))
}
```

## How to Select $\mu$ in the Poisson Prior?

```{r, Upsilon, echo = FALSE, cache = TRUE}
par(mar = c(4, 4, 1, 1) + .1, las = 1)
SEQ <- seq(from = 0, to = 204, by = 4)
Omega <- 0:10
plot(SEQ, sapply(SEQ, FUN = function(s) sum(Omega * Pr(Omega, Upsilon = s))), type = "l",
     xlab = expression(Upsilon1), ylab = "Expectation of First Roll")
```

## Using Bayes Rule with Bowling Data {.build}

```{r}
frames <- cbind(x_1 = c(9, 8, 10, 8, 7, 10, 9, 6, 9),
                x_2 = c(1, 1,  0, 2, 2,  0, 0, 3, 0))
```

> - Suppose that you observe the first $J = 9$ frames of bowling on the same person. Your
  posterior beliefs about $\Upsilon$ are given by
<font size="5">  
$$\Pr\left(\left.\Upsilon\right|x_{1,1},x_{1,2},\dots,x_{J,1},x_{J,2},\mu\right) = 
  \frac{\Pr\left(\left.\Upsilon\right|\mu\right)
        \Pr\left(\left.x_{1,1},x_{1,2},\dots,x_{J,1},x_{J,2}\right|\Upsilon\right)}
        {\Pr\left(x_{1,1},x_{1,2},\dots,x_{J,1},x_{J,2}\right)}$$
</font>
> - $\Pr\left(\left.\Upsilon\right|\mu\right) = \frac{\mu^\Upsilon e^{-\mu}}{\Upsilon!}$
  is the PMF for a Poisson distribution
> - What is $\Pr\left(\left.x_{1,1},x_{1,2},\dots,x_{J,1},x_{J,2}\right|\Upsilon\right)$?
> - Due to the independence of frames, $\Pr\left(\left.x_{1,1},x_{1,2},\dots,x_{J,1},x_{J,2}\right|\Upsilon\right) =$ 
  $\prod_{j=1}^J\Pr\left(\left.x_{j,1}\right|n = 10, \Upsilon\right)
                \Pr\left(\left.x_{j,2}\right|n=10-x_{j,1}, \Upsilon\right)$
> - What is $\Pr\left(x_{1,1},x_{1,2},\dots,x_{J,1},x_{J,2}\right)$?

## Denominator of Bayes Rule

- $\Pr\left(x_{1,1},x_{1,2},\dots,x_{J,1},x_{J,2}\right) = \mathbb{E}g\left(\Upsilon\right)$
  w.r.t. the prior PMF $\Pr\left(\left.\Upsilon\right|\mu\right)$,
  where $g\left(\Upsilon\right) = \Pr\left(\left.x_{1,1},x_{1,2},\dots,x_{9,1},x_{9,2}\right|\Upsilon\right)$.
  $g\left(\Upsilon\right)$ called the LIKELIHOOD function of $\Upsilon$ (evaluated at the observed data)
  
- The expected likelihood can be computed in this case as
$$\mathbb{E}g\left(\Upsilon\right) = 
\sum_{i=0}^\infty \Pr\left(\left.i\right|\mu\right)
\Pr\left(\left.x_{1,1},x_{1,2},\dots,x_{J,1},x_{J,2}\right|\Upsilon = i\right) =\\
\sum_{i=0}^\infty \frac{\mu^{i}e^{-\mu}}{i!}\prod_{j=1}^J
\Pr\left(\left.x_{j,1}\right|n = 10, \Upsilon = i\right)
\Pr\left(\left.x_{j,2}\right|n=10-x_{j,1}, \Upsilon = i\right)$$
  
- In practice, when evaluating an infinite sum we just stop once the terms get close enough to zero

## Calculating Posterior Probabilities in R

```{r, post, cache = TRUE, warning = FALSE}
mu <- 100                 # for example, can be any positive real number
Theta <- 0:207            # 207 is the biggest a laptop can handle
prior <- dpois(Theta, mu) # dpois() is the Poisson PMF
numer <- prior * sapply(Theta, FUN = function(i) { # sapply applies the given function
  J <- nrow(frames)                                # to each element of the first argument
  Pr_game <- Pr(frames[ , "x_1"], n = 10, Upsilon = i) * sapply(1:J, FUN = function(j)
             Pr(frames[j, "x_2"], n = 10 - frames[j, "x_1"], Upsilon = i))
  prod(Pr_game)
})
post <- numer / sum(numer)
```
```{r, eval = FALSE}
barplot(prior, names.arg = Theta, col = 2, ylim = range(post))
barplot(post, names.arg = Theta, col = 3)
```

## Comparison of Prior and Posterior Probabilities

```{r, echo = FALSE}
par(mfrow = 2:1, mar = c(4, 4, 1, 1), las = 1)
barplot(prior, names.arg = Theta, col = 2, xlim = c(80, 160), ylim = range(post),
        ylab = "Probability", xlab = expression(Upsilon1))
legend("topleft", legend = "Prior", lty = 1, col = 2, bg = "lightgrey")
barplot(post, names.arg = Theta, col = 3, xlim = c(80, 160),
        ylab = "Probability", xlab = expression(Upsilon1))
legend("topleft", legend = "Posterior", lty = 1, col = 3, bg = "lightgrey")
```

## Hypergeometric Distribution

- The binomial distribution corresponds to sampling WITH replacement
- The hypergeometric distribution corresponds to sampling WITHOUT replacement and
  has PMF $\Pr\left(\left.x\right|N,K,n\right) =
  \frac{{K \choose x}{N - K \choose n - x}}{{N \choose n}}$ where
  
    - $N$ is the (finite) size of the set being drawn from
    - $K$ is the number of successes in that finite set
    - $n$ is the number of times you draw without replacement

> - What is the probability of drawing two cards from a deck with the same value?
> - Intuitively, the probability of any pair should be
  $\frac{4}{52}\times\frac{3}{51} = \frac{1}{221}$ and there are $13$ ways to do that
  so `13 * dhyper(x = 2, 4, 52 - 4, 2)`

## Categorical Distribution

* The categorical distribution over $\Omega = \{1,2,\dots,K\}$ has a PMF
  $\Pr\left(\left.x\right|\pi_1,\pi_2,\dots,\pi_K\right) =
  \prod_{k=1}^K \pi_k^{\mathbb{I}\left(x=k\right)}$ where the parameters satisfy

    1. $\pi_k \geq 0 \forall k$
    2. $\sum_{k=1}^K \pi_k = 1$

* The categorical distribution is a generalization of the Bernoulli distribution to the
  case where there are $K$ categories rather than merely failure vs. success
* To draw randomly from it, you can do `sample(Omega, size = 1, prob = c(pi_1, pi_2, ..., pi_K))`
* You can make each $\pi_k$ a function of predictors in a regression model

## Multinomial Distribution

* The multinomial distribution over $\Omega = \{0,1,\dots,n\}$ has a PMF
  $\Pr\left(\left.x\right|\pi_1,\pi_2,\dots,\pi_K\right) =
  n!\prod_{k=1}^K \frac{\pi_k^{x_k}}{x_k!}$ where the parameters satisfy
  $\pi_k \geq 0 \forall k$, $\sum_{k=1}^K \pi_k = 1$, and $n = \sum_{k=1}^K x_k$

* The multinomial distribution is a generalization of the binomial distribution to the case that
  there are $K$ possibilities rather than merely failure vs. success
* The multinomial distribution is the count of $n$ independent categorical random variables
  with the same $\pi_k$ values
* Can draw from it with `rmultinom(1, size = n, prob = c(pi_1, pi_2, ..., pi_K))`
* Categorical is a special case where $n = 1$

## Objectivity and Subjectivity

- Under weak and not particularly controversial assumptions, Bayesian inference is THE objective way
  to update your beliefs about (functions of) $\theta$ in light of new data $y_1, y_2, \dots, y_N$
- Nevertheless, the Bayesian approach is labeled subjective because it does not say what your beliefs about 
  $\theta$ should be before you receive $y_1, y_2, \dots, y_N$
- Thus, if you currently believe something absurd about $\theta$ now, your beliefs about $\theta$ will
  merely be less absurd after updating them with $y_1, y_2, \dots, y_N$
- The big problem is not that people believe wrong things now, but that they do not update their 
  beliefs about $\theta$ according to Bayesian principles when they observe $y_1, y_2, \dots, y_N$
- In fact, in some situations, observing data that contradicts people's previous beliefs makes them
  believe in their wrong beliefs more strongly
- Bayesian principles are also used in formal models, but as an assumption about how people
  should behave rather than a behavioral description

## (Dis)Advantages of Bayesian Inference

- Bayesian inference remains useful in situations other paradigms specialize in:
    - Experiments: What are your beliefs about the ATE after seeing the data?
    - Repeated designs: Bayesian estimates have correct frequentist properties
    - Predictive modeling: If you only care about predictions, use the 
      posterior predictive distribution
- Bayesian inference is very useful when you are using the results to make a decision
  or take an action; other paradigms are not
- Bayesian inference is orders of magnitude more difficult for your computer because
  it is attempting to answer a more ambitious question
- The Bayesian approach is better suited for convincing yourself of something than
  convincing other people
