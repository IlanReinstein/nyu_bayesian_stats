---
title: Bayes Rule with Continuous Random Variables
author: Ben Goodrich
date: "`r format(Sys.time(), '%B %d, %Y')`"
autosize: true
header-includes:
   - \usepackage{amsmath}
   - \usepackage{color}
output:
  ioslides_presentation:
    widescreen: true
editor_options: 
  chunk_output_type: console
---


```{r, setup, include = FALSE}
options(width = 90)
library(knitr)
knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 4, .1, .1), las = 1)  # smaller margin on top and right
})
```

## Obligatory Disclosure

* Ben is an employee of Columbia University, which has received several research grants to develop Stan
* Ben is also a manager of GG Statistics LLC, which uses Stan for business purposes
* According to Columbia University 
  [policy](https://research.columbia.edu/content/conflict-interest-and-research), any such employee who 
  has any equity stake in, a title (such as officer or director) with, or is expected to earn at least 
  $\$5,000.00$ per year from a private company is required to disclose these facts in presentations

> - I do not have any connection to Mapp Biopharmaceutical, which we are going to use as an example today

## Beta Distribution

- The Beta distribution for $\pi \in \Theta = \left[0,1\right]$ has two positive shape parameters 
  $a$ and $b$ and its PDF involves the Beta function 
  $B\left(a,b\right) = \frac{\Gamma\left(a\right) \Gamma\left(b\right)}{\Gamma\left(a + b\right)}$:
$$f\left(\left.\pi\right|a,b\right) = \frac{1}{B\left(a,b\right)} \pi^{a - 1} \left(1 - \pi\right)^{b - 1}
\propto \pi^{a - 1} \left(1 - \pi\right)^{b - 1}$$
- Its expectation is $\mu = \frac{a}{a + b}$ &
  mode is $M = \frac{a - 1}{a + b - 2}$ but only exists if $a,b > 1$
- Its median, $m \approx \frac{a - \frac{1}{3}}{a + b - \frac{2}{3}}$, always exists but 
  approximation assumes $a,b > 1$
- Given $M, m \in \left(0,1\right)$, you can [solve](https://www.wolframalpha.com/input/?i=Reduce+function) 
  for $a > 1$ and $b > 1$
    - $a = \frac{m\left(4M - 3\right) + M}{3\left(M - m\right)}$ while
    $b = \frac{m\left(1 - 4M\right) + 5M - 2}{3\left(M - m\right)}$
    - But $m$ must be between $\frac{1}{2}$ and $M$ in order for $a > 1$ and $b > 1$

## Matching the Prior Predictive Distribution in Stan

```{r echo = FALSE, comment = ""}
cat(readLines("ebola_rng.stan"), sep = "\n")
```

## Updating Beliefs about the Success Probability

```{r message = FALSE, results = "hide"}
rstan::expose_stan_functions("ebola_rng.stan") # puts Stan functions into R's workspace
M <- 2 / 3; m <- 0.635; a <- (m * (4 * M - 3) + M) / (3 * (M - m)) 
b <- (m * (1 - 4 * M) + 5 * M - 2) / (3 * (M - m))
N <- 7L; y <- 5L; S <- 10000L; post <- ebola_rng(S, M, m, N, y) # call as an R function
```

```{r, small.mar = TRUE, fig.height=3, fig.width=10}
plot(post, y = (1:S) / S, type = "l", xlim = 0:1, ylab = "CDF of pi")
curve(pbeta(pi, a, b), from = 0, to = 1, add = TRUE, col = 2, lty = 2, xname = "pi")
legend("topleft", legend = c("Posterior", "Prior"), col = 1:2, lty = 1:2)
```

## Numerator of Bayes Rule

```{r, echo = FALSE, comment = ""}
cat(readLines("numerator.stan"), sep = "\n")
```
```{r, results = "hide"}
rstan::expose_stan_functions("numerator.stan")
```
```{r}
(denom <- integrate(numerator, lower = 0, upper = 1, M = M, m = m, N = N, y = y)$value)
```

## Same Info from the Probability Density Functions

```{r, fig.height=4, fig.width=10, small.mar = TRUE}
curve(numerator(pi, M, m, N, y) / denom, from = 0, to = 1, xname = "pi", ylab = "PDF of pi")
curve(dbeta(pi, a, b), from = 0, to = 1, col = 2, lty = 2, xname = "pi", add = TRUE)
lines(density(post, from = 0, to = 1),   col = 3, lty = 3)
legend("topleft", legend = c("Posterior", "Prior", "Approximate"), col = 1:3, lty = 1:3)
```

## Deriving the Posterior Distribution Analytically

> - Survivals are Binomial with probability $\pi$. Thus, BEFORE you see the results
$f\left(\left.y\right|N,\pi\right)={N \choose y}\pi^{y}\left(1-\pi\right)^{N-y}$
  while the Beta PDF is again
  $f\left(\left.\pi\right|a,b\right) = \frac{1}{B\left(a,b\right)} \pi^{a - 1} \left(1 - \pi\right)^{b - 1}$,
  so $B\left(a,b\right) = \int_0^1 \pi^{a - 1} \left(1 - \pi\right)^{b - 1} d\pi$
> - AFTER $y = 5$ out of $N = 7$ people were cured, what are your beliefs about $\pi$?
$$f\left(\left.\pi\right|a,b,N,y\right) = \frac{f\left(\left.\pi\right|a,b\right) L\left(\pi;N,y\right)}
{\int_0^1 f\left(\left.\pi\right|a,b\right) L\left(\pi;N,y\right) d\pi} \propto \\
\pi^{a - 1} \left(1 - \pi\right)^{b - 1} \pi^{y}\left(1-\pi\right)^{N-y}
= \pi^{a + y - 1} \left(1 - \pi\right)^{b + N - y - 1} = \pi^{a^\ast - 1} \left(1 - \pi\right)^{b^\ast - 1}$$
where $a^{\ast}=a+y$ and $b^{\ast}=b+N-y$
> - $f\left(\left.\pi\right|a^\ast,b^\ast\right)$ has the kernel of a Beta PDF and therefore the normalizing constant must be the reciprocal of $B\left(a^\ast,b^\ast\right)$

## Checking the Posterior CDF {.smaller}

```{r, fig.height = 4, fig.width=10, small.mar = TRUE}
plot(post, y = (1:S) / S, type = "l", xlim = 0:1, xlab = expression(pi), ylab = "CDF of pi")
curve(pbeta(pi, a, b), from = 0, to = 1, add = TRUE, col = 2, lty = 2, xname = "pi")
curve(pbeta(pi, a + y, b + N - y), from = 0, to = 1, add = TRUE, col = 3, lty = 3, xname = "pi")
legend("topleft", legend = c("From draws", "Prior", "Analytical"), col = 1:3, lty = 1:3)
```

## Checking the Posterior PDF {.smaller}

```{r, fig.height = 3, fig.width=10, small.mar = TRUE}
c(exact = (a + y) / (a + y + b + N - y), approximate = mean(post)) # posterior expectation
curve(numerator(pi, M, m, N, y) / denom, from = 0, to = 1, xname = "pi", ylab = "PDF of pi")
curve(dbeta(pi, a, b), from = 0, to = 1, col = 2, lty = 2, xname = "pi", add = TRUE)
curve(dbeta(pi, a + y, b + N - y), from = 0, to = 1, col = 3, lty = 3, xname = "pi", add = TRUE)
legend("topleft", legend = c("Numerical", "Prior", "Analytical"), col = 1:3, lty = 1:3)
```

## Properties of this Posterior Distribution

- The first approach illustrates a key point: The posterior PDF is the function closest to the
  prior PDF that satisfies a constraint given by the observed data
- Posterior expectation is between the prior expectation and sample mean
```{r}
c(prior = a / (a + b), posterior = (a + y) / (a + y + b + N - y), sample = y / N)
```
- As $N \uparrow \infty$ with $a$ and $b$ fixed, the posterior mean approaches $\frac{y}{N}$
- It does not matter if the data arrives one observation
  at a time, all at once, or somewhere in between. After $N$ tries and $y$ successes your posterior
  distribution will be the same, namely Beta with parameters $a^\ast = a + y$ and $b^\ast = b + N - y$,
  and contains all the information available from past data
- Ergo, you can use your posterior PDF as your prior PDF for the next dataset

