---
title: Probability with Continuous Random Variables
author: Ben Goodrich
date: "`r format(Sys.time(), '%B %d, %Y')`"
autosize: true
header-includes:
   - \usepackage{amsmath}
   - \usepackage{color}
output:
  ioslides_presentation:
    widescreen: true
---
<style type="text/css">
slides > slide:not(.nobackground):after {
  content: '';
}
</style>

```{r, setup, include = FALSE}
options(width = 90)
library(knitr)
knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 4, .1, .1), las = 1)  # smaller margin on top and right
})
```

## Obligatory Disclosure

* Ben is an employee of Columbia University, which has received several research grants to develop Stan
* Ben is also a manager of GG Statistics LLC, which uses Stan for business purposes
* According to Columbia University 
  [policy](https://research.columbia.edu/content/conflict-interest-and-research), any such employee who 
  has any equity stake in, a title (such as officer or director) with, or is expected to earn at least 
  $\$5,000.00$ per year from a private company is required to disclose these facts in presentations

> - But you should install the C++ toolchain and RStan R package anyway; see
https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started

## Hypergeometric Confusion

- The hypergeometric distribution corresponds to sampling WITHOUT replacement and
  has PMF $\Pr\left(\left.x\right|N,K,n\right) =
  {K \choose x}{N - K \choose n - x}{N \choose n}^{-1}$ where
  
    - $N$ is the (finite) size of the set being drawn from
    - $K$ is the number of successes in that finite set
    - $n$ is the number of times you draw without replacement
- The `dhyper` function in R parameterizes the hypergeometric PMF differently:

    - `x` is the number of successes sought in `k` draws
    - `m` is the number of successes in the set (deck, urn, etc.)
    - `n` is the number of failures in the set (deck, urn, etc.)
    - `k` is the number of times you draw from the set

- The probability of being dealt, for example, two tens from a deck is just
`dhyper(x = 2, m = 4, n = 52 - 4, k = 2)` $\approx 0.004525$

## Probability and Cumulative Mass Functions

- $\Pr\left(\left.x\right|\boldsymbol{\theta}\right)$ is a Probability Mass Function (PMF) 
over a discrete $\Omega$ that may depend on some parameter(s) $\boldsymbol{\theta}$ and the 
Cumulative Mass Function (CMF) is 
$\Pr\left(\left.X\leq x\right|\boldsymbol{\theta}\right)=\sum\limits_{i = \min\{\Omega\} }^x\Pr\left(\left.i\right|\boldsymbol{\theta}\right)$
- In the first roll of bowling, some simplification implies 
$\Pr\left(X\leq x\right) = \frac{ -1 + \mathcal{F}_{x+2}}{- 1 + \mathcal{F}_{n+2}}$
```{r}
source("https://tinyurl.com/y93srfmp") # code from week 1 to define F() and Omega
CMF <- function(x, n = 10) return( (- 1 + F(x + 2)) / (- 1 + F(n + 2)) )
round(CMF(Omega), digits = 4)
```
- How do we know this CMF corresponds to our PMF 
$\Pr\left(\left.x\right|n\right) = \frac{\mathcal{F}_{x}}{- 1 + \mathcal{F}_{n+2}}$?

## PMF is the Rate of Change in the CMF

```{r, echo=FALSE, fig.height=6,fig.width=9}
par(mar = c(5,4,0.5,0.5) + .1, las = 1)
cols <- rainbow(11)
x <- barplot(CMF(Omega), xlab = "Number of pins", ylab = "Probability of knocking down at most x pins", 
             col = cols, density = 0, border = TRUE)[,1]
for(i in 0:9) {
  j <- i + 1L
  points(x[j], CMF(i), col = cols[j], pch = 20)
  segments(x[j], CMF(i), x[j + 1L], CMF(j), col = cols[j], lty = 2)
}
abline(h = 1, lty = "dotted")
points(x[11], 1, col = cols[11], pch = 20)
```

## Cumulative Density Functions {.build}

> - Now $\Omega$ is an interval; e.g. $\Omega=\mathbb{R}$, $\Omega=\mathbb{R}_{+}$,
$\Omega=\left(a,b\right)$, etc.
> - $\Omega$ has an infinite number of points, so $\Pr\left(X = x\right) \downarrow 0$
> - $\Pr\left(X\leq x\right)$ is called the Cumulative Density Function (CDF) from $\Omega$ to 
$\left(0,1\right)$
> - No conceptual difference between a CMF and a CDF except emphasis on
whether $\Omega$ is discrete or continuous so we use 
$F\left(\left.x\right|\boldsymbol{\theta}\right)$ for both
```{r, echo = FALSE, fig.height=3, fig.width=9, small.mar = TRUE}
curve(plogis(x), from = -4, to = 4, ylab = "F(x) of Standard Logistic")
```

## The Standard Logistic CDF and PDF

- E.g., CDF of the standard logistic distribution over $\Omega = \mathbb{R}$ is $F\left(x\right) = \frac{1}{1+e^{-x}}$

> - $\Pr\left(a<X\leq x\right)=F\left(\left.x\right|\boldsymbol{\theta}\right)-F\left(\left.a\right|\boldsymbol{\theta}\right)$
as in the discrete case
> - If $x=a+h$, $\frac{F\left(\left.x\right|\boldsymbol{\theta}\right)-F\left(\left.a\right|\boldsymbol{\theta}\right)}{x-a}=\frac{F\left(\left.a+h\right|\boldsymbol{\theta}\right)-F\left(\left.a\right|\boldsymbol{\theta}\right)}{h}$
is the slope of a line segment
> - If we then let $h\downarrow0$, $\frac{F\left(\left.a+h\right|\boldsymbol{\theta}\right)-F\left(\left.a\right|\boldsymbol{\theta}\right)}{h}\rightarrow\frac{\partial F\left(\left.a\right|\boldsymbol{\theta}\right)}{\partial a}\equiv f\left(\left.x\right|\boldsymbol{\theta}\right)$
is still the RATE OF CHANGE in $F\left(\left.x\right|\boldsymbol{\theta}\right)$ at $x$
> - The derivative of the CDF $F\left(x\right)$ is called the Probability
Density Function (PDF) and denoted $f\left(x\right)$, which is always positive because the CDF increases
> - $f\left(x\right)$ is NOT a probability but is used like a PMF
> - What is slope of $F\left(x\right) = \frac{1}{1 + e^{-x}}$ at $x$?
> - [Answer](https://www.wolframalpha.com/input/?i=partial+derivative):
  $\frac{\partial}{\partial x}F\left(x\right) = 
  \frac{-1}{\left(1+e^{-x}\right)^2} \times \frac{\partial}{\partial x}e^{-x} = 
  \frac{-e^{-x}}{\left(1+e^{-x}\right)^2} \times \frac{\partial -x}{\partial x} = 
  \frac{e^{-x}}{\left(1+e^{-x}\right)^2} \geq 0$

## The Standard Normal CDF and Its Slope

Standard normal CDF over $\Omega = \mathbb{R}$ is
$\Phi(x) = \frac{1}{2} + \phi\left(x\right) S\left(x\right)$
where $\phi\left(x\right) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$,
$S\left(x\right) = \sum_{n=0}^\infty \frac{x^{2n+1}}{\left(2n + 1\right)!!}$, and
$a!!$ is the "double factorial" function for a non-negative integer $a$ such that 
$0!! = 1$, $1!! = 1$, and else $a!! = a \times \left(a - 2\right)!!$. What is 
the slope of $\Phi\left(x\right)$ at $x$?

>- 
$$\frac{\partial}{\partial x}\Phi\left(x\right) = \phi\left(x\right) S^\prime\left(x\right) + 
\phi^\prime\left(x\right) S\left(x\right) = 
\phi\left(x\right)\sum_{n=0}^\infty \frac{\left(2n + 1\right)x^{2n}}{\left(2n + 1\right)!!} -\\
\phi\left(x\right)x\sum_{n=0}^\infty \frac{x^{2n+1}}{\left(2n + 1\right)!!} =
\phi\left(x\right)\sum_{n=0}^\infty \frac{\left(2n + 1\right)x^{2n} - x^{2n+2}}{\left(2n + 1\right)!!} =\\
\phi\left(x\right)
\left(\frac{1 - x^2}{1}+\frac{3x^2 - x^4}{3 \times 1} + \frac{5x^4 - x^6}{5\times3\times1} \dots\right) = 
\phi\left(x\right)$$

## CDF and PDF of the Standard Normal Distribution

```{r, fig.height=4.5, fig.width=9, small.mar = TRUE}
curve(pnorm(x), from = -3, to = 3, ylim = c(-1,1), ylab = "") # CDF; what is the median?
curve(dnorm(x), add = TRUE, col = "red", lty = "dashed")      # PDF; what is the mode?
curve(x * dnorm(x), add = TRUE, col = 3, lty = "dotted")      # g function being integrated
legend("topleft", legend = c("CDF", "PDF", "xPDF"), col = 1:3, lty = 1:3, bg = "lightgrey")
```


## Expectations of Functions of a Continuous RV

- Let $g\left(X\right)$ be a function of a continuous $X \in \Omega$
- The probability that $X$ is in the interval $\left[x,x+dx\right]$ is 
$f\left(\left.x\right|\boldsymbol{\theta}\right)dx$ where
$dx$ is essentially the smallest non-neglible piece of $X$
- The expectation of $g\left(X\right)$, if it exists (which it may not), is defined as
$$\mathbb{E}g\left(X\right) = 
\int_{\min \Omega}^{\max \Omega} 
g\left(x\right)f\left(\left.x\right|\boldsymbol{\theta}\right)dx = 
G\left(\boldsymbol{\theta}\right)\bigg\rvert_{x = \min \Omega}^{x = \max \Omega}$$
- [Integrals](https://www.wolframalpha.com/input/?i=definite+integral) are usually impossible but we
  can use simulations to approximate them arbitrarily well. Still need to understand integrals conceptually as area.
- Columbia students can [download](https://cuit.columbia.edu/content/mathematica) Mathematica for free
- If $g\left(X\right)=X$, $\mathbb{E}X=\mu$ is "the" expectation and if $g\left(X\right)=\left(X-\mu\right)^{2}$, $\mathbb{E}\left[\left(X-\mu\right)^{2}\right]=\mathbb{E}\left[X^2\right] - \mu^2$ $= \sigma^{2}$ is the variance

## Moments of a Standard Normal Distribution

> - Note that the Standard Normal PDF only depends on the square of $x$, so
$$\mu = \int_{-\infty}^\infty{x \phi\left(x\right) dx}
      = \int_{-\infty}^0{x \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} dx} + 
        \int_{0}^\infty{x \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} dx} = 0$$
> - Let $\color{magenta}{y = \frac{x^2}{2}}$ so that $\color{blue}{\sqrt{2y} = x}$ and
$\color{red}{dy = x dx}$. Then, the variance is given by 
$$\sigma^2 = \int_{-\infty}^\infty{\left(x - 0\right)^2 \phi\left(x\right) dx} = 
2\int_{0}^\infty{\color{blue}{x} \frac{1}{\sqrt{2\pi}}e^{-\color{magenta}{\frac{x^2}{2}}} 
\color{red}{xdx}} =\\
\frac{2}{\sqrt{2\pi}}\int_{0}^\infty{\color{blue}{\sqrt{2y}}e^{-\color{magenta}{y}} \color{red}{dy}} =
\frac{2}{\sqrt{\pi}}\int_{0}^\infty{\color{blue}{y}^{\frac{3}{2} - 1}
e^{-\color{magenta}{y}} \color{red}{dy}} = 
\frac{2}{\sqrt{\pi}} \Gamma\left(\frac{3}{2}\right) = 1$$
> - $\Gamma\left(z\right) = \int_{0}^\infty{y^{z-1}e^{-y} dy}$ is a very
  important special function that 
  is a continuous generalization of $\left(z + 1\right)!$ and
  is implemented as `gamma(z)` in R

## Shift and Scale Transformations

> - If $Z$ is distributed standard normal & $X\left(Z\right) = \mu + \sigma Z$, 
  what's the PDF of $X$?
> - Answer: Note that $Z\left(X\right) = \frac{X - \mu}{\sigma}$. Since this is a monotonic
  transformation
$$\Pr\left(X\leq x\right) = \Pr\left(Z\leq z\left(x\right)\right) = \Phi\left(z\left(x\right)\right) \\
\frac{\partial}{\partial{x}}\Phi\left(z\left(x\right)\right) = 
\frac{\partial \Phi\left(z\right)}{\partial{z}} \times \frac{\partial z\left(x\right)}{\partial{x}} = 
\phi\left(z\left(x\right)\right) \frac{1}{\sigma} = 
\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}$$
> - $\mathbb{E}X = \mu + \sigma \mathbb{E}Z = \mu$ and
$\mathbb{E}\left[\left(X - \mu\right)^2\right] = \mathbb{E}\left[\left(\sigma Z\right)^2\right] = 
\sigma^2\mathbb{E}\left[Z^2\right] = \sigma^2$
> - Thus, 
$f\left(\left.x\right|\mu,\sigma\right) = 
\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}$ 
is the PDF of the general normal distribution with expectation $\mu$ and standard deviation $\sigma$ 
as parameters
> - The normal distribution is one of several in the location-scale family, where such
  transformations only change the location and scale of the distribution

## Nonlinear but Monotonic Transformations

> - If $Z$ is distributed normal with expectation $\mu$ and standard deviation $\sigma$
  and $X\left(Z\right) = e^Z$, what is the PDF of $X$? Hint: $\Pr\left(X \leq x\right) = \Pr\left(Z \leq z\left(x\right)\right)$
> - Answer: Note that $Z\left(X\right) = \ln X$ and $\frac{\partial}{\partial x}z\left(x\right) = \frac{1}{x}$ so
$f_X\left(\left.x\right|\mu,\sigma\right) = f_Z\left(\left.z\left(x\right)\right|\mu,\sigma\right) \times 
\frac{\partial}{\partial x}z\left(x\right) = 
\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{\ln\left(x\right)- \mu}{\sigma}\right)^2}$ 
is the PDF of the lognormal distribution over $\Omega = \mathbb{R}_+$
> - $\mu$ and $\sigma$ are parameters but NOT the expectation and standard deviation of $X$, due to the
  nonlinearity of the antilog transformation. It can be shown that $\mathbb{E}X = e^{\mu + \frac{1}{2}\sigma^2}$
  and $\mathrm{Var}\left(X\right) = \left(-1 + e^{\sigma^2}\right)e^{2\mu + \sigma^2}$.

## Scale Transformations of Exponential Variates

> - Standard exponential distribution over $\Omega = \mathbb{R}_+$ has CDF $F\left(x\right) = 1 - e^{-x}$
> - Its PDF is obviously $f\left(x\right) = \frac{\partial}{\partial x}F\left(x\right) = e^{-x}$, which
  must integrate to $1$
> - $\mathbb{E}X = \int_{0}^\infty xe^{-x}dx = -\left(x+1\right)e^{-x}
\bigg\rvert_{x = 0}^{x \rightarrow \infty} \rightarrow
    -\infty e^{-\infty} + e^{0} \rightarrow 1$
> - What is $\mathrm{Var}\left(X\right)$?
> - $\int_{0}^\infty \left(x - 1\right)^2 e^{-x}dx =
     \int_{0}^\infty x^2 e^{-x}dx - 2\int_{0}^\infty xe^{-x}dx + \int_{0}^\infty e^{-x}dx =\\
     -\left(x^2 + 2x + 2\right)e^{-x}\bigg \rvert_{x = 0}^{x \rightarrow \infty} - 2 \times 1 + 1 \rightarrow 1$
> - If $X$ is distributed standard exponential and $Y = \mu X$, what is the PDF of $Y$?
> - Answer: $\Pr\left(X\leq x\right) = \Pr\left(Y \leq y\left(x\right)\right)$, so
  $f\left(\left.y\right|\mu\right) = \frac{\partial 1 - e^{-\frac{y}{\mu}}}{\partial y} = 
  \frac{1}{\mu}e^{-\frac{y}{\mu}}$
> - You will often see this with the substitution $\lambda = \frac{1}{\mu}$. What are $\mathbb{E}Y$ & $\mathrm{Var}\left(Y\right)$?


## Bivariate Normal Distribution

If $\Pr\left(\left.X \leq x \bigcap Y \leq y\right|\boldsymbol{\theta}\right) = 
F\left(\left.x,y\right|\boldsymbol{\theta}\right)$ is a biviariate CDF, then the
bivariate PDF is
$\frac{\partial^2}{\partial x \partial y}F\left(\left.x,y\right|\boldsymbol{\theta}\right)$.
This generalizes beyond two dimensions. The PDF of the bivariate normal distribution over 
$\Omega = \mathbb{R}^2$ has five parameters:
$$f\left(\left.x,y\right|\mu_X,\mu_Y,\sigma_X,\sigma_Y,\rho\right) =\\
\frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}}e^{-\frac{1}{2\left(1-\rho^2\right)}
\left(\left(\frac{x - \mu_X}{\sigma_X}\right)^2 + 
\left(\frac{y - \mu_Y}{\sigma_Y}\right)^2 - 
2\rho\frac{x - \mu_X}{\sigma_X}\frac{y - \mu_Y}{\sigma_Y}\right)} = \\
\frac{1}{\sigma_X\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu_X}{\sigma_X}\right)^2} \times
\frac{1}{\color{blue}{\sigma_Y\sqrt{1-\rho^2}}\sqrt{2\pi}}e^{-\frac{1}{2}
\left(\frac{y - \left(\color{red}{\mu_y + \frac{\sigma_X}{\sigma_Y}\rho\left(x-\mu_x\right)}\right)}
{\color{blue}{\sigma_Y\sqrt{1-\rho^2}}}\right)^2},$$ where the first term is a marginal normal PDF and the
second is a conditional normal PDF
w/ parameters $\color{red}{\mu = \mu_Y + \frac{\sigma_X}{\sigma_Y}\rho\left(x-\mu_x\right)}$ &
$\color{blue}{\sigma = \sigma_Y\sqrt{1-\rho^2}}$.

## Where Is this Class Going?

- We usually think of parameters as being continuous but contained in a parameter (sub)space $\Theta$
- If you cut a continuous RV into a categorical RV, you could apply Bayes Rule
- If you take the limit as the number of cuts $\uparrow \infty$ you get Bayes Rule for continuous
  random variables
  $$f\left(\left.\theta\right|y,a,b\right) = \frac{
  f\left(\left.\theta\right|a,b\right) f\left(\left.y\right|\theta\right)}
  {f\left(\left.y\right|a,b\right)}$$
- Iff you have data, $y$, then $L\left(\theta;y\right)$ is the same expression as 
  $f\left(\left.y\right|\theta\right)$ but is
  a mathematical function of $\theta$ called the likelihood function that can be evaluated
  at any $\theta \in \Theta$ but only at the OBSERVED $y$
- By choosing functions for the numerator, you can (in principle) work out
  what $f\left(\left.y\right|a,b\right) = 
  \int_{\Theta}f\left(\left.\theta\right|a,b\right)L\left(\theta;y\right)d\theta$
  evaluates to complete Bayes Rule